Index: main.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nfrom utils import *\nfrom classifiers.MultivariateGaussianClassifier import *\nfrom classifiers.NaiveBayesClassifier import *\nfrom classifiers.TiedCovarianceGaussianClassifier import *\nfrom classifiers.TiedDiagCovGaussianClassifier import *\nfrom classifiers.LogisticRegression import *\nfrom classifiers.LinearSVM import *\nfrom classifiers.KernelSVM import *\nfrom classifiers.GaussianMixtureModel import *\nfrom tabulate import tabulate\n\nlabels_map = {\n    0: 'Not a Pulsar',\n    1: 'Pulsar'\n}\n\nfeatures_map = {\n    0: 'Mean of the integrated profile',\n    1: 'Standard deviation of the integrated profile',\n    2: 'Excess kurtosis of the integrated profile',\n    3: 'Skewness of the integrated profile',\n    4: 'Mean of the DM-SNR curve',\n    5: 'Standard deviation of the DM-SNR curve',\n    6: 'Excess kurtosis of the DM-SNR curve',\n    7: 'Skewness of the DM-SNR curve'\n}\n\n\ndef load_data(filepath):\n    data = []\n    labels = []\n    with open(filepath) as f:\n        for line in f:\n            fields = line.split(',')\n            data.append([float(feature) for feature in fields[0: 8]])\n            labels.append(int(fields[8]))\n    data = numpy.array(data).T  # transpose to have the features on the rows and the samples on the columns\n    labels = numpy.array(labels)\n    return data, labels\n\n\ndef plot_hist(D, L, folder='hist'):\n    D_index_L = [D[:, L == i] for i in set(L)]\n\n    for i in features_map.keys():\n        plt.figure()\n        plt.xlabel(features_map[i])\n        for index, data in enumerate(D_index_L):\n            plt.hist(data[i, :], bins=20, density=True, ec='black', alpha=0.5, label=labels_map[index])\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig('./plots/' + folder + '/hist_%d.png' % i)\n    plt.show()\n\n\ndef plot_scatter(D, L, folder='scatter'):\n    D_index_L = [D[:, L == i] for i in set(L)]\n\n    for i in features_map.keys():\n        for j in features_map.keys():\n            if i == j:\n                continue\n            plt.figure()\n            plt.xlabel(features_map[i])\n            plt.ylabel(features_map[j])\n            for index, data in enumerate(D_index_L):\n                plt.scatter(data[i, :], data[j, :], label=labels_map[index], alpha=0.5)  # red\n            plt.legend()\n            plt.tight_layout()\n            plt.savefig('./plots/' + folder + '/scatter_%d_%d.png' % (i, j))\n        plt.show()\n\n\ndef plot_heatmap(D, folder='heatmap', subtitle='', color='YlGn'):\n    corr_coef = numpy.corrcoef(D)\n\n    fig, ax = plt.subplots()\n    ax.imshow(corr_coef, cmap=color)\n    for i in range(len(features_map)):\n        for j in range(len(features_map)):\n            ax.text(j, i, str(round(corr_coef[i, j], 1)), ha=\"center\", va=\"center\", color=\"r\")\n\n    fig.tight_layout()\n    plt.savefig('./plots/' + folder + '/corr_coeff_' + subtitle + '.png')\n    plt.show()\n\n\ndef gaussianize(X, Z):\n    # Compute rank transformed features of Z over X (training samples)\n    Y = []\n    N = X.shape[1]\n    M = Z.shape[1]\n    for i, x in enumerate(X):\n        y = numpy.array([x[Z[i, j] < x].shape[0] for j in range(M)])  # equivalent to sum ones if Z[i, j] < X[i, :]\n        y = (y + 1) / (N + 2)  # this avoid +inf and -inf with ppf\n        Y.append(y)\n    # Return percent point function (inverse of the cumulative distribution function)\n    return norm.ppf(numpy.array(Y))\n\n\ndef covariance(D):\n    N = D.shape[1]\n    mu = vcol(D.mean(axis=1))\n    DC = D - mu\n    C = numpy.dot(DC, DC.T) / N\n    return C\n\n\ndef PCA(D, m):\n    C = covariance(D)\n    s, U = numpy.linalg.eigh(C)\n    P = U[:, ::-1][:, 0:m]\n    DP = numpy.dot(P.T, D)\n    return DP\n\n\ndef k_fold(D, L, K, Classifier, prior):\n    if K <= 1 or K > D.shape[1]:\n        raise Exception(\"K-Fold : K should be > 1 and <= \" + str(D.shape[1]))\n    nTest = int(D.shape[1] / K)\n    nTrain = D.shape[1] - nTest\n    numpy.random.seed(0)\n    idx = numpy.random.permutation(D.shape[1])\n    # duplicate idx\n    idx = numpy.concatenate((idx, idx))\n\n    n_classes = len(set(L))\n    errors = 0\n    for i in range(K):\n        start = i * nTest\n        idxTrain = idx[start: start + nTrain]\n        idxTest = idx[start + nTrain: start + nTrain + nTest]\n\n        DTR = D[:, idxTrain]\n        DTE = D[:, idxTest]\n        LTR = L[idxTrain]\n        LTE = L[idxTest]\n\n        classifier = Classifier(DTrain=DTR, LTrain=LTR)\n        predicted, _ = classifier.predict(DTest=DTE, LTest=LTE, prior=prior)\n        errors += predicted[LTE != predicted].shape[0]\n\n    return errors / (nTest * K)\n\n\ndef compute_confusion_matrix(true, predicted):\n    K = len(set(numpy.concatenate((true, predicted))))\n    confusion_matrix = numpy.zeros((K, K))\n\n    for i in range(len(true)):\n        confusion_matrix[predicted[i]][true[i]] += 1\n\n    return confusion_matrix\n\n\ndef DCFu(prior, cfn, cfp, confusion_matrix):\n    FNR = confusion_matrix[0, 1] / sum(confusion_matrix[:, 1])\n    FPR = confusion_matrix[1, 0] / sum(confusion_matrix[:, 0])\n    DCFu = prior * cfn * FNR + (1 - prior) * cfp * FPR\n    return DCFu\n\n\ndef DCF(prior, cfn, cfp, confusion_matrix):\n    DCFu_ = DCFu(prior, cfn, cfp, confusion_matrix)\n    Bdummy = min(prior * cfn, (1 - prior) * cfp)\n    return DCFu_ / Bdummy\n\n\ndef min_DCF(llr, labels, prior, cfn, cfp):\n    scores = numpy.sort(llr)\n\n    mindcf = None\n    for threshold in scores:\n        predicted = 0 + (llr > threshold)\n        confusion_matrix_min_dcf = compute_confusion_matrix(labels, predicted)\n        DCF_ = DCF(prior, cfn, cfp, confusion_matrix_min_dcf)\n        mindcf = mindcf if mindcf is not None and mindcf <= DCF_ else DCF_\n\n    return mindcf\n\n\ndef k_fold_min_DCF(D, L, K, Classifier, prior, args=()):\n    if K <= 1 or K > D.shape[1]:\n        raise Exception(\"K-Fold : K should be > 1 and <= \" + str(D.shape[1]))\n    nTest = int(D.shape[1] / K)\n    nTrain = D.shape[1] - nTest\n    numpy.random.seed(0)\n    idx = numpy.random.permutation(D.shape[1])\n    # duplicate idx\n    idx = numpy.concatenate((idx, idx))\n\n    n_classes = len(set(L))\n    mindcf = 0\n    for i in range(K):\n        start = i * nTest\n        idxTrain = idx[start: start + nTrain]\n        idxTest = idx[start + nTrain: start + nTrain + nTest]\n\n        DTR = D[:, idxTrain]\n        DTE = D[:, idxTest]\n        LTR = L[idxTrain]\n        LTE = L[idxTest]\n\n        print(\"K = \" + str(i + 1) + \", preClassifier\")\n        classifier = Classifier(DTR, LTR, *args)\n        print(\"K = \" + str(i + 1) + \", postClassifier\")\n        mindcf += min_DCF(classifier.llr(DTE), LTE, prior, 1, 1)\n        print(\"K = \" + str(i + 1) + \", postMin_DCF\")\n\n    return mindcf / K\n\n\ndef compute(name, i, p, d, k_fold_min_DCF, D, LTR, c, args, j):\n    print(name + \" - prior = \" + str(p) + \" - data id = \" + str(d))\n    a = round(k_fold_min_DCF(D, LTR, K=5, Classifier=c, args=args, prior=p), 3)\n    print(\"min_DCF = \" + str(a))\n    return a\n\n\nif __name__ == '__main__':\n    DTR, LTR = load_data('./data/Train.txt')\n\n    print_plots = False\n\n    try:\n        DTR_G = numpy.load('./data/TrainGAU.npy')\n    except FileNotFoundError:\n        DTR_G = gaussianize(DTR, DTR)\n        numpy.save('./data/TrainGAU.npy', DTR_G)\n\n    if print_plots:\n        plot_hist(DTR, LTR)\n        plot_scatter(DTR, LTR)\n\n        plot_hist(DTR_G, LTR, folder='hist_GAU')\n        plot_scatter(DTR_G, LTR, folder='scatter_GAU')\n\n        plot_heatmap(DTR, subtitle='all', color='binary')\n        plot_heatmap(DTR[:, LTR == 1], subtitle='pulsar', color='Blues')\n        plot_heatmap(DTR[:, LTR == 0], subtitle='not_pulsar', color='Greens')\n\n    DTR_G_PCA_7 = PCA(DTR_G, 7)\n    DTR_G_PCA_6 = PCA(DTR_G, 6)\n    DTR_G_PCA_5 = PCA(DTR_G, 5)\n    DTR_G_PCA_4 = PCA(DTR_G, 4)\n\n    # print(k_fold(DTR_PCA_4, LTR, 5, MultivariateGaussianClassifier, prior=vcol(numpy.array([0.1, 0.9]))))\n    # print(k_fold(DTR_PCA_4, LTR, 5, NaiveBayesClassifier, prior=vcol(numpy.array([0.1, 0.9]))))\n    # print(k_fold(DTR_PCA_4, LTR, 5, TiedCovarianceGaussianClassifier, prior=vcol(numpy.array([0.1, 0.9]))))\n    # print(k_fold(DTR_PCA_4, LTR, 5, TiedDiagCovGaussianClassifier, prior=vcol(numpy.array([0.1, 0.9]))))\n\n    classifier_name = numpy.array([\n        'Full-Cov',\n        'Diag-Cov',\n        'Tied Full-Cov',\n        'Tied Diag-Cov'\n    ])\n    classifiers = numpy.array([\n        # MultivariateGaussianClassifier,\n        # NaiveBayesClassifier,\n        # TiedCovarianceGaussianClassifier,\n        # TiedDiagCovGaussianClassifier\n    ])\n\n    priors = numpy.array([0.5, 0.1, 0.9])\n    mindcf = numpy.zeros((classifiers.shape[0], priors.shape[0]))\n    data = [DTR, DTR_G]  # [DTR_G, DTR_G_PCA_7, DTR_G_PCA_6, DTR_G_PCA_5, DTR]\n\n    for d, D in enumerate(data):\n        for i, c in enumerate(classifiers):\n            for j, p in enumerate(priors):\n                print(classifier_name[i] + \" - prior = \" + str(p) + \" - data id = \" + str(d))\n                mindcf[i, j] = round(k_fold_min_DCF(D, LTR, K=5, Classifier=c, prior=p), 3)\n                print(\"min_DCF = \" + str(mindcf[i, j]))\n        if classifiers.shape[0] > 0:\n            table = numpy.hstack((vcol(classifier_name), mindcf))\n            print(tabulate(table, headers=[\"\"] + list(priors), tablefmt='fancy_grid'))\n\n    import multiprocessing as mp\n\n    pool = mp.Pool(mp.cpu_count())\n\n    classifier_name = numpy.array([\n        'Log Reg'\n    ])\n    classifiers = numpy.array([\n        LogisticRegression\n    ])\n\n    priors = numpy.array([0.5, 0.1, 0.9])\n    lamb = numpy.array([10 ** i for i in range(-5, 5)])\n    mindcf = numpy.zeros((classifiers.shape[0], priors.shape[0], lamb.shape[0]))\n    data = [DTR, DTR_G]  # [DTR_G, DTR_G_PCA_7, DTR_G_PCA_6, DTR_G_PCA_5, DTR]\n\n    for d, D in enumerate(data):\n        for i, c in enumerate(classifiers):\n            for j, p in enumerate(priors):\n                mindcf[i, j] = pool.starmap(compute, [(classifier_name[i], i, p, d, k_fold_min_DCF, D, LTR, c, (l, None), j) for k, l in enumerate(lamb)])\n\n        # table = numpy.hstack((vcol(classifier_name), mindcf))\n        # print(tabulate(table, headers=[\"\"] + list(priors), tablefmt='fancy_grid'))\n\n    print(mindcf[0, 0])  # [0.862 0.866 0.904 0.928 0.929 0.909 0.871 0.752 0.424 1.   ]\n    print(mindcf[0, 1])  # [0.91  0.913 0.945 0.975 0.979 0.977 0.955 0.914 0.838 1.   ]\n    print(mindcf[0, 2])  # [ ]\n    # print(mindcf[1, 0])\n    # print(mindcf[1, 1])\n    # print(mindcf[1, 2])\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.py b/main.py
--- a/main.py	(revision 58c295f5d98603cc1ec325d763824a529ddaa37b)
+++ b/main.py	(date 1623114641334)
@@ -171,14 +171,16 @@
 
 
 def min_DCF(llr, labels, prior, cfn, cfp):
-    scores = numpy.sort(llr)
+    scores = numpy.sort(llr[::100])
 
     mindcf = None
+    err_rate = 1
     for threshold in scores:
-        predicted = 0 + (llr > threshold)
-        confusion_matrix_min_dcf = compute_confusion_matrix(labels, predicted)
-        DCF_ = DCF(prior, cfn, cfp, confusion_matrix_min_dcf)
-        mindcf = mindcf if mindcf is not None and mindcf <= DCF_ else DCF_
+        predicted = numpy.array(0 + (llr > threshold))
+        if predicted[predicted != labels].shape[0] / predicted.shape[0] < err_rate:
+            confusion_matrix_min_dcf = compute_confusion_matrix(labels, predicted)
+            DCF_ = DCF(prior, cfn, cfp, confusion_matrix_min_dcf)
+            mindcf = mindcf if mindcf is not None and mindcf <= DCF_ else DCF_
 
     return mindcf
 
@@ -205,18 +207,15 @@
         LTR = L[idxTrain]
         LTE = L[idxTest]
 
-        print("K = " + str(i + 1) + ", preClassifier")
         classifier = Classifier(DTR, LTR, *args)
-        print("K = " + str(i + 1) + ", postClassifier")
         mindcf += min_DCF(classifier.llr(DTE), LTE, prior, 1, 1)
-        print("K = " + str(i + 1) + ", postMin_DCF")
 
     return mindcf / K
 
 
 def compute(name, i, p, d, k_fold_min_DCF, D, LTR, c, args, j):
     print(name + " - prior = " + str(p) + " - data id = " + str(d))
-    a = round(k_fold_min_DCF(D, LTR, K=5, Classifier=c, args=args, prior=p), 3)
+    a = round(k_fold_min_DCF(D, LTR, K=2, Classifier=c, args=args, prior=p), 3)
     print("min_DCF = " + str(a))
     return a
 
@@ -310,3 +309,8 @@
     # print(mindcf[1, 0])
     # print(mindcf[1, 1])
     # print(mindcf[1, 2])
+
+    # [0.546 0.913 0.866 0.861 0.842 0.284 0.853 0.568 0.849 0.998]
+    # [0.705 0.983 0.978 0.951 0.924 0.413 0.998 0.788 0.926 1.]
+    # [1.03  1.022 1.019 1.044 1.035 0.956 1.009 0.906 0.994 1.022]
+
